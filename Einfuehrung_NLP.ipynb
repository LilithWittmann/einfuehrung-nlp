{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden werden anhand von Beispielen mithilfe der Software Libraries:\n",
    "- spacy - eine Toolsammlung um Probleme im Bereich NLP mithilfe von vortrainierten Deeplearning zu lösen\n",
    "- nltk - eher akademische Sammlung von Tools im Bereich NLP\n",
    "- TextBlob - abstraktion für einige spezifische deutsche Corpora (z.B. ready to use sentiment analysis…)\n",
    "\n",
    "einige elementare Methoden im Bereich NLP vorgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "\n",
    "#load spacy de corpora\n",
    "nlp = spacy.load(\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook dient als Ergänzung zu den Slides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"//lilith.slides.com/lilith/deck-31/embed?token=ivKQJ292\" width=\"576\" height=\"420\" scrolling=\"no\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe src=\"//lilith.slides.com/lilith/deck-31/embed?token=ivKQJ292\" width=\"576\" height=\"420\" scrolling=\"no\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die meisten Tasks werden wir spacy benutzen, da es einerseits das aus meiner Sicht \"industry-grade\" tool für NLP und außerdem sehr einfach zu bedienen ist. In Spacy können wir das komplette lingustische preprocessing eines Satzes mit einem einzelnen Kommando erledigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wie wird das Wetter morgen?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Wie wird das Wetter morgen?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit ```nlp(\"<text>\")``` durchläuft der Satz eine ganze Pipeline an funktionen und gibt den komplett annotierten Text als ein document zurück. Die Pipelines sehen dabei für jede Sprache etwas anders aus beinhalten grundsätzlich allerdings immer folgende Koponenten:\n",
    "- Tokenizer\n",
    "- POS-Tagging\n",
    "- Dependency Parsing\n",
    "- NER-Tagging\n",
    "\n",
    "Die Pipeline für Deutsch sieht so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x7f0c1c512438>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x7f0c1c5243b8>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x7f0c1c524410>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](https://spacy.io/assets/img/pipeline.svg)\n",
    "Im Folgenden werden wir uns mit den einzelnen Funktionen innerhalb dieser Pipeline beschäftigen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Um Text sinnvoll weiterverarbeiten zu können, müssen wir ihn erst in seine einzelnen Bestandteile zerlegen. Das können einelne Wörter, Satzteile oder Sätze (Sentence Boundary Detection) sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wie', 'wird', 'das', 'Wetter', 'morgen', 'in', 'Hamburg', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.WordPunctTokenizer().tokenize(\"Wie wird das Wetter morgen in Hamburg?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wie', 'wird', 'das', 'Wetter', 'morgen', 'in', 'Hamburg', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in nlp(\"Wie wird das Wetter morgen in Hamburg?\")]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augenscheinlich machen sowohl der NLTK als auch der spacy tokenizer hier genau das selbe. Allerdings funktioniert der NLTK Tokenizer im Hintergrund quasi mit einem Regex, während spacy dafür erst den Satz selbst nach Spaces separiert und dann mit einzelnen Regeln je nach Sprache auch auf Edge-Cases wie Fremdwörter im Deutschen, die mit Bindestrich geschrieben werden, rücksicht nimmt. \n",
    "\n",
    "![caption](https://spacy.io/assets/img/tokenization.svg)\n",
    "\n",
    "Auch wenn das jetzt für Sprachen mit Lateinischen Alphabet erstmal relativ einfach aussieht, gibt es einige Sprachen (Chinesisch, Thailändisch, …) in denen das eine deutlich schwierigere Aufgabe ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Davon',\n",
       " 'hatte',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'Müller',\n",
       " 'kein',\n",
       " 'Back',\n",
       " '-',\n",
       " 'up',\n",
       " 'gemacht',\n",
       " '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.WordPunctTokenizer().tokenize(\"Davon hatte Dr. Müller kein Back-up gemacht!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Davon', 'hatte', 'Dr.', 'Müller', 'kein', 'Back-up', 'gemacht', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in nlp(\"Davon hatte Dr. Müller kein Back-up gemacht!\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einen guten Tokenizer braucht man normalerweise nicht nur bei fast allen Anwendungsfällen im Bereich Natural Language Processing, sondern auch vielen weiteren Bereichen wie z.B. Suchsystemen. Außerdem sollte man bedenken, das fehler in diesem Bereich problematisch für alle weiteren Arbeitsschritte sein können.\n",
    "\n",
    "Beliebte weitere Fallstricke:\n",
    "- Abkürzungen wie Dr., Fr., … sollten als ein Token gesehen werden und nicht zu einer Satztrennung führen\n",
    "- Zahlen, Telefonnummern, feststehende Begriffe, … (je nach Level auf dem man tokenized vgl. High-Level-Tokenization)\n",
    "- Abkürzungen wie I'm, You're, …\n",
    "\n",
    "(https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech tagging (POS)\n",
    "\n",
    "Beim POS tagging geht es darum einen Satz in seine grammatikalischen Satzteile zu zerlegen. Es gibt dabei wieder unterschiedliche herangehensweisen wie das mithilfe von Software passieren kann. Spacy verwendet dabei statistische Verfahren (Markov Models). So ist die wahrscheinlichkeit relativ hoch das nach einem \"dem\" ein Substantiv im Dativ folgt. Diese Wahrscheinlichkeitskalkulationen passieren jetzt über mehrere Wörter hinweg, was dann zu einem relativ guten Ergebnis führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"950\" height=\"137.0\" style=\"max-width: none; height: 137.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Der</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">Arzt</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">NN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">hilft</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VVFIN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">dem</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">Patienten</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">$.</tspan>\n",
       "</text>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Der Arzt hilft dem Patienten.')\n",
    "spacy_rendering = {\"words\": [], \"arcs\": []}\n",
    "for token in doc:\n",
    "    spacy_rendering[\"words\"].append({\"text\": token.text, \"tag\": token.tag_})\n",
    "HTML(displacy.render(spacy_rendering, style='dep', manual=True, options={\"compact\": True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für POS tagging alleine gibt es nicht so viele Usecases, allerdings ist es die Grundlage für Dependency Parsing und Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depedency Parsing\n",
    "Beim dependency parsing geht es darum, herauszufinden welches Wort sich wie auf welches andere Wort bezieht und hilft so auch dabei Haupt- und Nebensätze zu separieren um die Kernaussage eines Satzes herauszufinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"925\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Der</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Arzt</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">hilft</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">dem</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Patienten.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">sb</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">da</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Der Arzt hilft dem Patienten.')\n",
    "HTML(displacy.render(doc, style='dep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim NER geht es darum Informationen (insb. feststehende Begriffe) aus Texten zu extrahieren und zu klassifizieren. Dies passiert heute im Normalfall auf basis der annotierten Daten (POS; depedency tree) und "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Wie wird das Wetter morgen in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Hamburg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "?</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Wie wird das Wetter morgen in Hamburg?')\n",
    "HTML(displacy.render(doc, style='ent'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistische Methoden für NER ermöglichen uns auch für nicht im Trainingsset vorhandenen Namen/… Tags mit einer reltiv hohen Genauigkeit zu vergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Wer ist \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Alan Turing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "?</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Wer ist Alan Turing?')\n",
    "HTML(displacy.render(doc, style='ent'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Wer ist \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Lilith Wittmann\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       "?</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Wer ist Lilith Wittmann?')\n",
    "HTML(displacy.render(doc, style='ent'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer\n",
    "Ein Stemmer ist ein Tool, welches versucht anhand von einfachen Regeln ein Wort auf seinen Wortstamm zu kürzen (z.B. um festzustellen ob ein Wort eine Konjugation eines anderen ist). Das funktioniert im Englischen auch relativ gut, allerdings haben wir im Deutschen Probleme wie starke Verben die das ganze so ziemlich nutzlos machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mensch kind bad'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.GermanStemmer()\n",
    "# Dinge wie Umlaute in pluralformen sind regelbasiert noch machbar\n",
    "stemmer.stem(\"Menschen\") + \" \" + stemmer.stem(\"Kinder\") + \" \" + stemmer.stem(\"Bäder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ess ass gegess'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"essen\") + \" \" + stemmer.stem(\"aß\") + \" \" + stemmer.stem(\"gegessen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "Ein Lemmatizer arbeitet im Gegensatz (zumindest in der Trainingsphase) mit einem echten Wörterbuch und kann so den lexikalischen Wortstamm herausfinden. Das ermöglicht dann später auch funktionen wie das verändern der Konjugation eines Verbs beim Umstellen eines Satzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wie', 'werden', 'der', 'wettern', 'morgen', 'in', 'Hamburg', '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in nlp(\"Wie wird das Wetter morgen in Hamburg?\")] # funktioniert bei spacy noch nicht so perfekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lilith', 'essen', 'gestern', 'Hummus', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in nlp(\"Lilith aß gestern Hummus.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['essen', '-', 'essen', '-', 'essen']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in nlp(\"essen - aß - gegessen\")] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "Zumeist die häufigsten Wörter einer Sprache oder spezifischen Domäne, die in einem Text beim Weiterverarbeiten ignoriert werden sollen. Werden z.B. bei suchsystemen benutzt um nicht jedem Text, die einen Artikel der im Suchquery stand enthält, zu finden.\n",
    "\n",
    "Kann bei feststehenden Begriffen und Eigenenamen z.B. \"die Welle\" problematisch werden. Da empfiehlt sich dann Elemente die von einem NER erkannt wurden nicht als Stopwords zu markieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wie - False',\n",
       " 'wird - True',\n",
       " 'das - True',\n",
       " 'Wetter - False',\n",
       " 'morgen - True',\n",
       " 'in - True',\n",
       " 'Hamburg - False',\n",
       " '? - False']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"{} - {}\".format(token.text, token.is_stop) for token in nlp(\"Wie wird das Wetter morgen in Hamburg?\")] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline erweitern\n",
    "Es kann vorkommen, das man eigene Funktionen in einer bestehenden preprocessing Pipeline durchführen möchte. z.B. seinen eigenen NER Tagger benutzen oder Daten auf eine Bestimmte Art bereinigen. Auch das ist mit Spacy relativ einfach möglich.\n",
    "\n",
    "In diesem Fall wollen wir die Wortvectoren von einzelnen Wörtern zusammenfügen um einen kompletten Satz in seiner Vectorform erhalten zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'calculate_document_vectors']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def calculate_document_vectors(document):\n",
    "    \"\"\"calculates the vectors for the input sentence and adds it to the document\"\"\"\n",
    "    vectors = []\n",
    "    for token in document:\n",
    "        if token.vector.all() != None:\n",
    "            vectors.append(token.vector)\n",
    "    if len(vectors) > 0:\n",
    "        sum_vectors = np.sum(vectors, axis=0) / len(vectors)\n",
    "    else:\n",
    "        sum_vectors = np.zeros(spacy.vocab.vectors_length)\n",
    "    \n",
    "    # intialize extension variable\n",
    "    doc.set_extension('vector', default=True)\n",
    "    \n",
    "    document._.vector = sum_vectors\n",
    "    return document\n",
    "\n",
    "# add calculate_document_vectors as a pipeline step\n",
    "nlp.add_pipe(calculate_document_vectors, name='calculate_document_vectors', last=True)\n",
    "print(nlp.pipe_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.60005283e-01 -1.42031324e+00 -1.39692652e+00  1.54604244e+00\n",
      "  5.10063469e-01  2.53958797e+00 -7.85469413e-01 -1.67401409e+00\n",
      "  4.75193679e-01  1.52145612e+00  1.31106055e+00 -8.10774684e-01\n",
      " -2.86875105e+00  1.91085148e+00  1.41275108e+00  2.52005005e+00\n",
      " -6.02132738e-01 -8.42801630e-01  2.14502287e+00 -1.35398519e+00\n",
      "  6.15745842e-01 -7.12715834e-02 -7.52448320e-01  5.48410416e-02\n",
      " -1.91989970e+00  2.39742613e+00  3.35050249e+00 -4.06068563e-01\n",
      "  4.55959082e-01  1.89419657e-01 -1.08924103e+00 -1.30628419e+00\n",
      " -1.89369351e-01 -1.57305360e+00  2.04342890e+00  1.83883011e+00\n",
      "  4.97969836e-01  8.40657592e-01 -2.68162751e+00  1.74087620e+00\n",
      "  2.09340155e-02 -2.97767115e+00  1.30148733e+00  1.49778759e+00\n",
      "  1.05355740e+00  2.44767523e+00  2.92400861e+00  2.13427973e+00\n",
      " -1.04330087e+00  2.15468574e+00 -1.05976999e-01 -4.15641129e-01\n",
      " -1.32000005e+00 -2.37148643e-01 -1.66685748e+00 -7.34763265e-01\n",
      " -9.63998556e-01  1.15699506e+00  2.04573900e-01  1.46130383e+00\n",
      " -1.53127193e+00  2.83403844e-01  3.91660428e+00 -2.46368170e+00\n",
      "  3.18144202e-01 -1.43062520e+00  1.45626211e+00  4.19116080e-01\n",
      " -3.99365544e-01  1.98399639e+00  1.54147729e-01 -2.94134021e-01\n",
      "  1.09613872e+00 -1.15584016e-01  1.63897693e-01 -7.87351131e-01\n",
      "  2.52648330e+00  1.19782031e+00  1.14256036e+00 -9.85362411e-01\n",
      " -1.62805116e+00  2.55066347e+00 -6.74732089e-01 -5.90821028e-01\n",
      " -1.07800865e+00 -5.61172962e-02  5.00451088e-01  6.00086510e-01\n",
      "  9.09919977e-01  8.89382243e-01 -1.85997152e+00  1.97084999e+00\n",
      "  1.18120885e+00  9.93783325e-02 -2.21606779e+00 -9.41732287e-01\n",
      " -1.50971222e+00  3.99047494e-01 -1.16401315e+00  3.37845659e+00\n",
      " -8.29754770e-02 -2.71417952e+00 -5.65662086e-01  5.07841647e-01\n",
      " -1.60110331e+00 -6.76175714e-01  8.11968505e-01 -1.78887820e+00\n",
      " -1.32372880e+00 -3.50618422e-01  1.31344438e+00 -2.38284683e+00\n",
      "  8.12673926e-01 -1.19658184e+00 -2.28112793e+00  1.25784755e-01\n",
      " -1.56573784e+00  1.13140309e+00 -2.07569838e+00 -2.04581690e+00\n",
      " -4.69164163e-01 -1.73635280e+00  1.26944721e-01 -1.39957345e+00\n",
      " -1.03622699e+00  8.88132155e-01 -1.61781657e+00  1.22596276e+00\n",
      " -9.05075729e-01 -8.56131673e-01  8.75814974e-01 -5.12768686e-01\n",
      "  2.12758869e-01 -6.31083488e-01 -1.90557376e-01  1.61357626e-01\n",
      "  4.45232838e-01  2.48510823e-01  4.54515666e-01  3.11904550e-02\n",
      "  2.31315047e-01 -4.59768236e-01  2.58876085e-02  1.28942460e-01\n",
      " -1.08665012e-01 -2.80620545e-01 -7.65869498e-01 -1.32653117e-01\n",
      "  1.30786926e-01  1.07959211e-01 -6.14284396e-01 -6.12475157e-01\n",
      " -2.28158832e-02 -4.87620562e-01  4.58161533e-01  3.60990375e-01\n",
      "  9.52674985e-01 -5.87186515e-02 -6.47176802e-02  6.28486812e-01\n",
      " -4.99326408e-01 -6.48667157e-01  3.54559183e-01 -1.12334049e+00\n",
      "  2.21976250e-01 -1.88923046e-01  3.48358393e-01 -4.91696358e-01\n",
      " -1.90201029e-01 -7.58307949e-02  5.67526698e-01  6.37742102e-01\n",
      "  3.64350528e-01  6.42546475e-01 -6.80346668e-01  5.16722560e-01\n",
      "  3.37513089e-01  1.09445357e+00 -3.90959442e-01 -1.46636203e-01\n",
      " -6.13405883e-01  2.21485913e-01  1.47466630e-01 -3.06856722e-01\n",
      " -1.21427083e+00  3.08700621e-01  5.79630911e-01  6.06093109e-02\n",
      "  5.27034163e-01  5.06293416e-01 -3.29307765e-01 -2.72629082e-01\n",
      "  3.35256159e-01  1.39205360e+00  4.90612268e-01 -4.90035936e-02\n",
      "  3.51070255e-01  1.78386688e-01 -4.62298512e-01 -2.07255334e-02\n",
      " -4.33937937e-01  3.79405618e-02 -1.70517966e-01 -5.75634688e-02\n",
      "  2.18394771e-01  1.02938223e+00 -1.73926950e-01  1.88230574e-01\n",
      " -6.18307710e-01 -4.74815279e-01  3.20764720e-01 -1.01590931e-01\n",
      "  1.29519868e+00  9.04127598e-01  3.27372134e-01  1.22805044e-01\n",
      "  7.64107108e-02  4.14386749e-01 -3.99185240e-01 -3.53453130e-01\n",
      "  9.47154313e-02 -1.64084375e-01  8.98406386e-01 -3.89055192e-01\n",
      " -3.00418615e-01  4.42909479e-01  1.54268593e-01 -5.29937744e-01\n",
      " -6.17510229e-02  4.10543233e-02 -3.33232611e-01 -5.02346039e-01\n",
      " -8.38576138e-01 -5.79088211e-01  3.12688649e-01  8.45227182e-01\n",
      " -1.20065808e-02  5.88600874e-01 -3.84694636e-01 -8.21290076e-01\n",
      "  1.49526298e-01  1.77130938e-01  3.42192292e-01 -4.16439354e-01\n",
      " -1.65622339e-01  3.28787774e-01 -2.49187559e-01 -5.03308952e-01\n",
      " -6.23147249e-01 -7.82048345e-01  8.17758739e-01 -8.41517299e-02\n",
      " -5.41368902e-01  8.82460475e-01 -7.76300788e-01  2.64843404e-01\n",
      " -7.37379715e-02  6.68907389e-02 -8.67915899e-02 -4.48168293e-02\n",
      "  6.99454099e-02  1.43957853e-01 -2.08391137e-02  3.50570008e-02\n",
      "  1.35863274e-01  9.90174860e-02  1.98857471e-01  1.30694270e-01\n",
      " -1.43136382e-02  9.07081366e-02 -7.46962130e-02 -2.32559107e-02\n",
      "  1.03468671e-02 -3.17828879e-02 -5.99196665e-02 -1.43716767e-01\n",
      " -1.57558113e-01 -9.70379040e-02 -5.11258990e-02  2.00782623e-03\n",
      " -2.79026367e-02  9.24299881e-02 -5.69845997e-02 -1.55742913e-01\n",
      "  1.44273907e-01  1.91030040e-01  1.97948292e-02 -1.70441628e-01\n",
      " -6.28020316e-02  1.34252295e-01 -9.39539447e-02  3.43820974e-02\n",
      "  9.01590958e-02 -1.24914669e-01 -1.51364535e-01  9.30610448e-02\n",
      "  1.71762243e-01  7.69515783e-02 -4.45768684e-02 -2.63856221e-02\n",
      " -5.21544516e-02 -4.05178256e-02  3.30663547e-02 -6.25034869e-02\n",
      "  3.94927412e-02  6.42976314e-02  1.88938342e-02 -3.09279244e-02\n",
      "  1.01319849e-01  1.47233143e-01 -1.46555692e-01  8.97955820e-02\n",
      " -2.50729531e-01 -6.76137805e-02  4.48839739e-03 -3.15499865e-02\n",
      " -1.23901725e-01 -1.20699570e-01  1.20691601e-02 -8.24189112e-02\n",
      " -6.45920858e-02  1.62668750e-01  1.25091020e-02 -1.48320735e-01\n",
      " -1.10767350e-01  8.70944932e-04  3.20336595e-02  1.11503880e-02\n",
      "  6.36897683e-02 -9.48811248e-02  9.01842937e-02  5.28489426e-02\n",
      " -8.96446556e-02  4.07940224e-02  2.35887058e-02  1.71161920e-01\n",
      " -5.42640053e-02  1.32123992e-01  2.25731991e-02  6.07865229e-02\n",
      " -4.04992998e-02 -2.03926563e-02 -1.19728699e-01 -8.39930996e-02\n",
      " -2.11784899e-01  6.30674213e-02  3.56095284e-02  4.86765206e-02\n",
      " -1.12152532e-01  2.34080199e-03  1.40677206e-03 -1.47430841e-02\n",
      "  2.04582900e-01 -4.79762033e-02 -1.65307894e-04 -5.33822626e-02\n",
      "  6.61225170e-02  2.95837279e-02 -3.31010967e-02 -2.83900890e-02\n",
      "  9.50198695e-02 -9.21723098e-02  6.56619854e-03 -2.58665830e-02\n",
      " -4.04771790e-03 -1.03693798e-01 -8.17316175e-02  2.29227632e-01\n",
      "  7.89632127e-02  5.17809615e-02  1.62102878e-01  8.09446573e-02\n",
      "  6.33201450e-02 -1.12193204e-01  6.04744442e-02 -1.78679422e-01\n",
      "  1.22234739e-01 -7.03128651e-02 -7.63761848e-02 -2.18439490e-01\n",
      " -1.25569832e-02 -1.68645754e-02  9.13017839e-02 -6.69458210e-02]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wie wird das Wetter morgen in Hamburg?\")\n",
    "print(doc._.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geocoding**\n",
    "Außerdem wollen wir dierekt in der Pipeline alle als Orte erkannten Entities mit Geodaten Taggen um sie z.B. auf einer Karte darstellen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'calculate_document_vectors', 'geocode_locations']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def geocode_locations(document):\n",
    "    for key, entity in enumerate(document.ents):\n",
    "        if entity.label_ == \"LOC\":\n",
    "            document.ents[key].set_extension('location', default=None)\n",
    "            RESPONSE_STATUS_OK = 'OK'\n",
    "            GEOCODE_API_URL = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "            params = {'sensor': 'false', 'language': \"DE\", 'address': entity.text}\n",
    "            response = requests.get(GEOCODE_API_URL, params=params)\n",
    "            geo_response = response.json()\n",
    "            results = []\n",
    "            # print(geo_response['status'] )\n",
    "            if geo_response['status'] == RESPONSE_STATUS_OK:\n",
    "                location = {'lat': float(geo_response['results'][0]['geometry']['location']['lat']),\n",
    "                            'lon': float(geo_response['results'][0]['geometry']['location']['lng'])}\n",
    "                document.ents[key]._.location = location\n",
    "                time.sleep(2)#hack for google quota limits\n",
    "            \n",
    "    return document\n",
    "\n",
    "# add calculate_document_vectors as a pipeline step\n",
    "nlp.add_pipe(geocode_locations, name='geocode_locations', last=True)\n",
    "print(nlp.pipe_names) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraktion der geo entity aus unserem Beispielsatz…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat': 53.5510846, 'lon': 9.9936819}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"wie wird das Wetter morgen in Hamburg?\")\n",
    "print(doc.ents[0]._.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit jupyter können wir so außerdem super einfach die in texten vorkommende Orte auf einer Karte visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3238ef00e2274d95aef032833d2a4abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(basemap={'max_zoom': 19, 'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'attribution': 'Map …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker, \n",
    "    TileLayer, \n",
    "    basemaps\n",
    ")\n",
    "\n",
    "center = [51.381716, 10.304767]\n",
    "zoom = 5\n",
    "\n",
    "m = Map(center=center, zoom=zoom)\n",
    "\n",
    "\n",
    "doc = nlp(\"Mit der Völkerwanderung im 5. Jahrhundert verfielen die antiken Städte auf dem Gebiet des heutigen Deutschlands weitgehend. Nur zu Augsburg, Regensburg, Trier und Köln ist der durchgängige Bestand als Stadt gesichert. Die Anzahl der Städte in Mitteleuropa blieb bis ca. 1100 mit einigen Hundert noch sehr gering. Der weitaus größte Teil entstand in den folgenden 250 Jahren, als ab 1120 zahlreiche Gründungsstädte entstanden, meist durch einen Gründungsakt und Stadtentwurf. Etwa zum Beginn der Neuzeit, Anfang des 16. Jahrhunderts, bestanden als bedeutende Städte (neben weiteren heute v. a. niederländischen, französischen und belgischen) vorwiegend die Freien Reichsstädte und die Residenzstädte:\")\n",
    "\n",
    "for entity in doc.ents:\n",
    "    if entity.label_ == \"LOC\":\n",
    "        # print(entity.text)\n",
    "        if hasattr(entity._, 'location') and entity._.location != None:\n",
    "            #print([entity._.location[\"lat\"], entity._.location[\"lon\"]])\n",
    "            m += Marker(location=[entity._.location[\"lat\"], entity._.location[\"lon\"]])\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training einer SVM zur intent classification\n",
    "In vielen aktuellen QnA und chatbot Systemen versuchen wir anhand des vom User eingegebenen Satzes in Verbindung mit dem Gesprächskontext den intent hinter einer Aussage herauszufinden. \n",
    "\n",
    "Zum training haben wir soeben eine Funktion in spacy erweitert, um word vectoren auf Satzebene zu berechnen. Nehmen wir an, wir möchten eine SVM mit folgenden 3 Klassen trainieren:\n",
    "- greeting\n",
    "- frage nach dem wetter\n",
    "- sonstige nicht zuordnenbare aussagen\n",
    "\n",
    "Dafür haben nun jeweils 10 trainingssätze (was natürlich für einen echten usecases viel zu wenig ist, aber zu demozwecken ausreichend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {\n",
    "    \n",
    "    \"greeting\": [\n",
    "        \"hi\",\n",
    "        \"Hey\",\n",
    "        \"Hallo\",\n",
    "        \"Guten Tag\",\n",
    "        \"Guten Morgen\",\n",
    "        \"Guten Abend\",\n",
    "        \"Grüßgott\",\n",
    "        \"Na du\",\n",
    "        \"Ahoi\",\n",
    "        \"Glück auf\"\n",
    "    ],\n",
    "    \"weather\": [\n",
    "        \"Wie wird das Wetter heute?\",\n",
    "        \"Was sagt das Wetter\",\n",
    "        \"Wird es morgen in Hamburg regnen?\",\n",
    "        \"Wird es sonnig in Hamburg?\",\n",
    "        \"Ich will wissen ob morgen die sonne scheint?\",\n",
    "        \"Wird es morgen kalt?\",\n",
    "        \"Wie warm wird es morgen?\",\n",
    "        \"Wird es morgen in Berlin warm?\",\n",
    "        \"Wie warm wird es heute abend?\"\n",
    "        \"Wird es Freitag schneien?\"\n",
    "    ],\n",
    "    \"out_of_scope\": [\n",
    "        \"der atem meiner katze riecht nach Katzenfutter\",\n",
    "        \"Heute ist das Techcamp\",\n",
    "        \"In Hamburg sind die Mietpreise hoch\", \n",
    "        \"Heute abend gibts Pizza\",\n",
    "        \"Amazon alexa ist langweilig\",\n",
    "        \"f3oirf0\",\n",
    "        \"Sinnlos, Teuer einfach dumm - Vorratsdatenspeicherung\",\n",
    "        \"Langsam fallen mir keine sätze mehr ein\",\n",
    "        \"Warum muss ich soviel schreiben?\",\n",
    "        \"Ob das wirklich mir 10 sätzen funktioniert, wage ich zu bezweifeln\"\n",
    "    ]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes preprocessen wir all unsere Sätze und ersetzen spezifische NERs (z.B. locations) durch placeholder um die SVM z.B. nicht auf einzelne Städte zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n",
      "OVER_QUERY_LIMIT\n",
      "OK\n",
      "OVER_QUERY_LIMIT\n",
      "OK\n",
      "OVER_QUERY_LIMIT\n",
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "for label in training_data:\n",
    "    for key, item in enumerate(training_data[label]):\n",
    "        training_data[label][key] = nlp(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bücher und andere Resourcen\n",
    "- [**spacy Doku** ](https://spacy.io/usage/spacy-101)\n",
    "- [nltk doku](https://www.nltk.org/)\n",
    "- [Next Generation Natural Language Processing with Python - Alexis Rutherford](https://www.safaribooksonline.com/library/view/natural-language-processing/9781787285101/)\n",
    "- [Natural Language Processing with Python Cookbook](https://www.safaribooksonline.com/library/view/natural-language-processing/9781787289321/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
